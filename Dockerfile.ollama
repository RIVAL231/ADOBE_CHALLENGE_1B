# Stage 1: Download the Ollama model
FROM ollama/ollama AS model-downloader

# Start ollama server in background and pull the model
RUN nohup ollama serve > /dev/null 2>&1 & \
    sleep 15 && \
    OLLAMA_HOST=http://localhost:11434 ollama pull gemma3:1b && \
    sleep 5 && \
    pkill -f ollama

# Stage 2: Build the final Ollama container with the pre-downloaded model
FROM ollama/ollama

# Set the OLLAMA_HOST environment variable for accessibility
ENV OLLAMA_HOST "0.0.0.0"

# Copy the downloaded model from the first stage to the final container
# Ollama stores models in /root/.ollama/models by default
COPY --from=model-downloader /root/.ollama /root/.ollama

# Expose the default Ollama port
EXPOSE 11434

# Command to run Ollama when the container starts
CMD ["/bin/ollama", "serve"]
